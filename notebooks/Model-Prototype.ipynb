{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import yaml\n",
    "import torch\n",
    "import joblib\n",
    "import zipfile\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(value=None, filename=None):\n",
    "    if (value is not None) and (filename is not None):\n",
    "        joblib.dump(value=value, filename=filename)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Both value and filename must be provided\".capitalize())\n",
    "\n",
    "\n",
    "def load(filename=None):\n",
    "    if filename is not None:\n",
    "        return joblib.load(filename=filename)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Filename must be provided\".capitalize())\n",
    "\n",
    "\n",
    "def device_init(device=\"cuda\"):\n",
    "    if device == \"cuda\":\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    elif device == \"mps\":\n",
    "        return torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def config():\n",
    "    with open(\"../../config.yml\", \"r\") as file:\n",
    "        return yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_path: str = None,\n",
    "        image_channels: int = 3,\n",
    "        image_size: int = 128,\n",
    "        batch_size: int = 64,\n",
    "        split_size: float = 0.25,\n",
    "    ):\n",
    "        self.image_path = image_path\n",
    "        self.image_channels = image_channels\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.split_size = split_size\n",
    "\n",
    "        self.X = list()\n",
    "        self.Y = list()\n",
    "\n",
    "        try:\n",
    "            self.CONFIG = config()\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred while loading config file: \", e)\n",
    "        else:\n",
    "            self.RAW_DATA_PATH = self.CONFIG[\"path\"][\"RAW_DATA_PATH\"]\n",
    "            self.PROCESSED_DATA_PATH = self.CONFIG[\"path\"][\"PROCESSED_DATA_PATH\"]\n",
    "\n",
    "    def dataset_split(self, X: list, y: list):\n",
    "        if isinstance(X, list) and isinstance(y, list):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=self.split_size, random_state=42\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"X_train\": X_train,\n",
    "                \"X_test\": X_test,\n",
    "                \"y_train\": y_train,\n",
    "                \"y_test\": y_test,\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"X and y must be list\".capitalize())\n",
    "\n",
    "    def transforms(self):\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((self.image_size, self.image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.CenterCrop((self.image_size, self.image_size)),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def unzip_folder(self):\n",
    "        if os.path.exists(self.RAW_DATA_PATH):\n",
    "            with zipfile.ZipFile(self.image_path, \"r\") as zip:\n",
    "                zip.extractall(self.RAW_DATA_PATH)\n",
    "\n",
    "        else:\n",
    "            raise FileNotFoundError(\"RAW Path not found\".capitalize())\n",
    "\n",
    "    def extract_features(self):\n",
    "        self.directory = os.path.join(self.RAW_DATA_PATH, \"dataset\")\n",
    "        self.categories = config()[\"dataloader\"][\"labels\"]\n",
    "\n",
    "        for category in tqdm(self.categories):\n",
    "            image_path = os.path.join(self.directory, category)\n",
    "\n",
    "            for image in os.listdir(image_path):\n",
    "                image = os.path.join(image_path, image)\n",
    "\n",
    "                if (image is not None) and (image.endswith((\".jpg\", \".png\", \".jpeg\"))):\n",
    "                    image = cv2.imread(image)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    image = self.transforms()(Image.fromarray(image))\n",
    "                    label = self.categories.index(category)\n",
    "\n",
    "                    self.X.append(image)\n",
    "                    self.Y.append(label)\n",
    "\n",
    "                else:\n",
    "                    print(\"Image not found\".capitalize())\n",
    "\n",
    "        assert len(self.X) == len(\n",
    "            self.Y\n",
    "        ), \"Image size and Label size not equal\".capitalize()\n",
    "\n",
    "        try:\n",
    "            dataset = self.dataset_split(X=self.X, y=self.Y)\n",
    "        except TypeError as e:\n",
    "            print(\"An error occured: \", e)\n",
    "        except Exception as e:\n",
    "            print(\"An error occured: \", e)\n",
    "\n",
    "        else:\n",
    "            return dataset\n",
    "\n",
    "    def create_dataloader(self):\n",
    "        dataset = self.extract_features()\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            dataset=list(zip(dataset[\"X_train\"], dataset[\"y_train\"])),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        valid_dataloader = DataLoader(\n",
    "            dataset=list(zip(dataset[\"X_test\"], dataset[\"y_test\"])),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        for value, filename in [\n",
    "            (train_dataloader, \"train_dataloader\"),\n",
    "            (valid_dataloader, \"valid_dataloader\"),\n",
    "        ]:\n",
    "            dump(\n",
    "                value=value,\n",
    "                filename=os.path.join(\n",
    "                    self.PROCESSED_DATA_PATH, \"{}.pkl\".format(filename)\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        print(\"Dataloader is saved in the folder {}\".format(self.PROCESSED_DATA_PATH))\n",
    "\n",
    "    @staticmethod\n",
    "    def display_images():\n",
    "        FILES_PATH = config()[\"path\"][\"FILES_PATH\"]\n",
    "        PROCESSED_PATH = config()[\"path\"][\"PROCESSED_DATA_PATH\"]\n",
    "\n",
    "        os.makedirs(FILES_PATH, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(FILES_PATH):\n",
    "            plt.figure(figsize=(20, 20))\n",
    "\n",
    "            dataloader = load(\n",
    "                filename=os.path.join(PROCESSED_PATH, \"train_dataloader.pkl\")\n",
    "            )\n",
    "\n",
    "            data, label = next(iter(dataloader))\n",
    "\n",
    "            labels = config()[\"dataloader\"][\"labels\"]\n",
    "\n",
    "            number_of_rows = len(data) // int(\n",
    "                math.sqrt(config()[\"dataloader\"][\"batch_size\"])\n",
    "            )\n",
    "            number_of_columns = len(data) // number_of_rows\n",
    "\n",
    "            for index, image in enumerate(data):\n",
    "                X = image.squeeze().permute(1, 2, 0).detach().cpu().numpy()\n",
    "                X = (X - X.min()) / (X.max() - X.min())\n",
    "\n",
    "                plt.subplot(2 * number_of_rows, 2 * number_of_columns, 2 * index + 1)\n",
    "                plt.imshow(X)\n",
    "                plt.title(labels[label[index]].capitalize())\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(FILES_PATH, \"image.png\"))\n",
    "            plt.show()\n",
    "\n",
    "            print(\"Images are saved in {}\".format(FILES_PATH))\n",
    "\n",
    "        else:\n",
    "            raise FileNotFoundError(\"The folder {} does not exist\".format(FILES_PATH))\n",
    "\n",
    "    @staticmethod\n",
    "    def dataset_details():\n",
    "        FILES_PATH = config()[\"path\"][\"FILES_PATH\"]\n",
    "        PROCESSED_PATH = config()[\"path\"][\"PROCESSED_DATA_PATH\"]\n",
    "\n",
    "        os.makedirs(FILES_PATH, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(FILES_PATH):\n",
    "            plt.figure(figsize=(20, 20))\n",
    "\n",
    "            train_dataloader = load(\n",
    "                filename=os.path.join(PROCESSED_PATH, \"train_dataloader.pkl\")\n",
    "            )\n",
    "            valid_dataloader = load(\n",
    "                filename=os.path.join(PROCESSED_PATH, \"valid_dataloader.pkl\")\n",
    "            )\n",
    "\n",
    "            train_data, _ = next(iter(train_dataloader))\n",
    "\n",
    "            dataset = pd.DataFrame(\n",
    "                {\n",
    "                    \"train_data\": [\n",
    "                        sum(actual.size(0) for actual, _ in train_dataloader)\n",
    "                    ],\n",
    "                    \"train_labels\": [\n",
    "                        sum(target.size(0) for _, target in train_dataloader)\n",
    "                    ],\n",
    "                    \"valid_data\": [\n",
    "                        sum(actual.size(0) for actual, _ in valid_dataloader)\n",
    "                    ],\n",
    "                    \"valid_labels\": [\n",
    "                        sum(target.size(0) for _, target in valid_dataloader)\n",
    "                    ],\n",
    "                    \"total_data\": [\n",
    "                        sum(actual.size(0) for actual, _ in train_dataloader)\n",
    "                        + sum(actual.size(0) for actual, _ in valid_dataloader)\n",
    "                    ],\n",
    "                    \"batch_size\": [train_data.size(0)],\n",
    "                    \"channels\": [train_data.size(1)],\n",
    "                    \"height\": [train_data.size(2)],\n",
    "                    \"width\": [train_data.size(3)],\n",
    "                },\n",
    "                index=[\"Dataset Details\"],\n",
    "            ).to_csv(os.path.join(FILES_PATH, \"dataset_details.csv\"))\n",
    "\n",
    "            print(\n",
    "                \"Dataset details saved to {}\".format(\n",
    "                    os.path.join(FILES_PATH, \"dataset_details.csv\").capitalize()\n",
    "                )\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise FileNotFoundError(\"The folder {} does not exist\".format(FILES_PATH))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Dataloader for ViT\".title())\n",
    "    parser.add_argument(\n",
    "        \"--image_path\",\n",
    "        type=str,\n",
    "        default=config()[\"dataloader\"][\"image_path\"],\n",
    "        help=\"Path to the image dataset\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--channels\",\n",
    "        type=int,\n",
    "        default=config()[\"dataloader\"][\"channels\"],\n",
    "        help=\"Number of channels in the image\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_size\",\n",
    "        type=int,\n",
    "        default=config()[\"dataloader\"][\"image_size\"],\n",
    "        help=\"Size of the image\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        default=config()[\"dataloader\"][\"batch_size\"],\n",
    "        help=\"Batch size for the dataloader\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--split_size\",\n",
    "        type=float,\n",
    "        default=config()[\"dataloader\"][\"split_size\"],\n",
    "        help=\"Split size for the dataloader\".capitalize(),\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    loader = Loader(\n",
    "        image_path=args.image_path,\n",
    "        image_channels=args.channels,\n",
    "        image_size=args.image_size,\n",
    "        batch_size=args.batch_size,\n",
    "        split_size=args.split_size,\n",
    "    )\n",
    "    # loader.unzip_folder()\n",
    "    loader.extract_features()\n",
    "    loader.create_dataloader()\n",
    "\n",
    "    try:\n",
    "        Loader.display_images()\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"An error is occcured\", e)\n",
    "    except Exception as e:\n",
    "        print(\"An error is occcured\", e)\n",
    "\n",
    "    try:\n",
    "        Loader.dataset_details()\n",
    "    except Exception as e:\n",
    "        print(\"An error is occcured\", e)\n",
    "    except Exception as e:\n",
    "        print(\"An error is occcured\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Encoder\n",
    "    1. Positional Encoding\n",
    "    2. Scaled Dot Product Attention\n",
    "    3. Multi Head Attention Layer \n",
    "    4. Layer Normalization \n",
    "    5. PointWise Feed Forward Neural Network\n",
    "    6. Encoder Block\n",
    "    7. Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self, sequence_length: int = 200, dimension: int = 512, constant: int = 10000\n",
    "    ):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.dimension = dimension\n",
    "        self.constant = constant\n",
    "\n",
    "        positional_encoding = torch.randn((self.sequence_length, self.dimension))\n",
    "\n",
    "        for pos in range(self.sequence_length):\n",
    "            for index in range(self.dimension):\n",
    "                if index % 2 == 0:\n",
    "                    positional_encoding[pos, index] = math.sin(\n",
    "                        pos / (self.constant ** ((2 * index) / self.dimension))\n",
    "                    )\n",
    "                else:\n",
    "                    positional_encoding[pos, index] = math.cos(\n",
    "                        pos / (self.constant ** ((2 * index) / self.dimension))\n",
    "                    )\n",
    "\n",
    "        self.positional_encoding = nn.Parameter(positional_encoding, requires_grad=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return self.positional_encoding.unsqueeze(0)[:, : x.size(1), :]\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Positional Encoding for the transformer\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sequence_length\",\n",
    "        type=int,\n",
    "        default=200,\n",
    "        help=\"Length of the sequence\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dimension\",\n",
    "        type=int,\n",
    "        default=config()[\"ViT\"][\"dimension\"],\n",
    "        help=\"Dimension of the positional encoding\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--constant\",\n",
    "        type=int,\n",
    "        default=10000,\n",
    "        help=\"Constant used in the positional encoding\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    positional_encoding = PositionalEncoding(\n",
    "        sequence_length=args.sequence_length,\n",
    "        dimension=args.dimension,\n",
    "        constant=args.constant,\n",
    "    )\n",
    "\n",
    "    assert positional_encoding(\n",
    "        torch.randn((40, args.sequence_length, args.dimension))\n",
    "    ).size() == (\n",
    "        1,\n",
    "        args.sequence_length,\n",
    "        args.dimension,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask=None\n",
    "):\n",
    "    if (\n",
    "        isinstance(query, torch.Tensor)\n",
    "        and isinstance(key, torch.Tensor)\n",
    "        and isinstance(value, torch.Tensor)\n",
    "    ):\n",
    "        result = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            result = torch.add(result, mask)\n",
    "\n",
    "        attention_weight = torch.softmax(input=result, dim=-1)\n",
    "\n",
    "        attention = torch.matmul(attention_weight, value)\n",
    "\n",
    "        assert (\n",
    "            attention.size() == query.size() == key.size() == value.size()\n",
    "        ), \"Sizes of inputs are not equal\".capitalize()\n",
    "\n",
    "        return attention\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"All inputs must be of type torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Scaled dot product attetion for transformer\".title()\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    batch_size = config()[\"ViT\"][\"batch_size\"]\n",
    "    nheads = config()[\"ViT\"][\"nheads\"]\n",
    "    dimension = config()[\"ViT\"][\"dimension\"]\n",
    "\n",
    "    query = torch.randn((batch_size, nheads, 200, dimension // nheads))\n",
    "    key = torch.randn((batch_size, nheads, 200, dimension // nheads))\n",
    "    value = torch.randn((batch_size, nheads, 200, dimension // nheads))\n",
    "    mask = torch.randint(0, 2, (batch_size, 200))\n",
    "\n",
    "    attention = scaled_dot_product_attention(\n",
    "        query=query,\n",
    "        key=key,\n",
    "        value=value,\n",
    "        mask=None,\n",
    "    )\n",
    "\n",
    "    assert attention.size() == (\n",
    "        batch_size,\n",
    "        nheads,\n",
    "        200,\n",
    "        dimension // nheads,\n",
    "    ), \"Sizes of inputs are not equal\".capitalize()\n",
    "\n",
    "    attention = scaled_dot_product_attention(\n",
    "        query=query,\n",
    "        key=key,\n",
    "        value=value,\n",
    "        mask=mask,\n",
    "    )\n",
    "\n",
    "    assert attention.size() == (\n",
    "        batch_size,\n",
    "        nheads,\n",
    "        200,\n",
    "        dimension // nheads,\n",
    "    ), \"Sizes of inputs are not equal\".capitalize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimension: int = 512,\n",
    "        nheads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.nheads = nheads\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "\n",
    "        assert (\n",
    "            self.dimension % self.nheads == 0\n",
    "        ), \"dimension must be divisible by nheads\".capitalize()\n",
    "\n",
    "        self.QKV = nn.Linear(\n",
    "            in_features=self.dimension, out_features=3 * self.dimension, bias=self.bias\n",
    "        )\n",
    "\n",
    "        self.layers = nn.Linear(\n",
    "            in_features=self.dimension, out_features=self.dimension, bias=self.bias\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            QKV = self.QKV(x)\n",
    "\n",
    "            self.query, self.key, self.value = torch.chunk(input=QKV, chunks=3, dim=-1)\n",
    "\n",
    "            assert (\n",
    "                self.query.size() == self.key.size() == self.value.size()\n",
    "            ), \"QKV must have the same size\".capitalize()\n",
    "\n",
    "            self.query = self.query.view(\n",
    "                self.query.size(0),\n",
    "                self.query.size(1),\n",
    "                self.nheads,\n",
    "                self.dimension // self.nheads,\n",
    "            )\n",
    "\n",
    "            self.key = self.key.view(\n",
    "                self.key.size(0),\n",
    "                self.key.size(1),\n",
    "                self.nheads,\n",
    "                self.dimension // self.nheads,\n",
    "            )\n",
    "\n",
    "            self.value = self.value.view(\n",
    "                self.value.size(0),\n",
    "                self.value.size(1),\n",
    "                self.nheads,\n",
    "                self.dimension // self.nheads,\n",
    "            )\n",
    "\n",
    "            self.query = self.query.permute(0, 2, 1, 3)\n",
    "            self.key = self.key.permute(0, 2, 1, 3)\n",
    "            self.value = self.value.permute(0, 2, 1, 3)\n",
    "\n",
    "            self.attention = scaled_dot_product_attention(\n",
    "                query=self.query,\n",
    "                key=self.key,\n",
    "                value=self.value,\n",
    "                mask=mask,\n",
    "            )\n",
    "\n",
    "            self.attention = self.attention.view(\n",
    "                self.attention.size(0),\n",
    "                self.attention.size(2),\n",
    "                self.attention.size(1) * self.attention.size(3),\n",
    "            )\n",
    "\n",
    "            assert (\n",
    "                self.attention.size() == x.size()\n",
    "            ), \"Attention output size does not match input size\"\n",
    "\n",
    "            return self.layers(self.attention)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"MultiHeadAttention Layer for the transformer\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dimension\",\n",
    "        type=int,\n",
    "        default=config()[\"ViT\"][\"dimension\"],\n",
    "        help=\"Dimension of the input tensor\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nheads\",\n",
    "        type=int,\n",
    "        default=config()[\"ViT\"][\"nheads\"],\n",
    "        help=\"Number of heads in the multihead attention layer\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\",\n",
    "        type=float,\n",
    "        default=config()[\"ViT\"][\"dropout\"],\n",
    "        help=\"Dropout rate for the multihead attention layer\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    batch_size = config()[\"ViT\"][\"batch_size\"]\n",
    "\n",
    "    attention = MultiHeadAttention(\n",
    "        dimension=args.dimension, nheads=args.nheads, dropout=args.dropout, bias=True\n",
    "    )\n",
    "\n",
    "    assert attention(torch.randn((batch_size, 200, args.dimension))).size() == (\n",
    "        batch_size,\n",
    "        200,\n",
    "        args.dimension,\n",
    "    ), \"MultiHeadAttention Layer is not working properly\".capitalize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, normalized_shape: int = 512, epsilon: float = 1e-05):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.ones((normalized_shape,)))\n",
    "        self.beta = nn.Parameter(torch.zeros((normalized_shape,)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            self.mean = torch.mean(input=x, dim=-1)\n",
    "            self.variance = torch.var(input=x, dim=-1)\n",
    "\n",
    "            self.mean = self.mean.unsqueeze(-1)\n",
    "            self.variance = self.variance.unsqueeze(-1)\n",
    "\n",
    "            return (\n",
    "                self.gamma * (x - self.mean) / torch.sqrt(self.variance + self.epsilon)\n",
    "                + self.beta\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Layer Normalization for the Transformer\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--normalized_shape\",\n",
    "        type=int,\n",
    "        default=config()[\"ViT\"][\"dimension\"],\n",
    "        help=\"The shape of the input tensor\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epsilon\",\n",
    "        type=float,\n",
    "        default=config()[\"ViT\"][\"eps\"],\n",
    "        help=\"The epsilon value for the variance\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    batch_size = config()[\"ViT\"][\"batch_size\"]\n",
    "\n",
    "    layer_norm = LayerNormalization(\n",
    "        normalized_shape=args.normalized_shape, epsilon=args.epsilon\n",
    "    )\n",
    "\n",
    "    assert layer_norm(torch.rand((batch_size, 200, args.normalized_shape))).size() == (\n",
    "        batch_size,\n",
    "        200,\n",
    "        args.normalized_shape,\n",
    "    ), \"Layer Normalization failed\".capitalize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PointWise Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int = 512,\n",
    "        out_features: int = 2048,\n",
    "        dropout: float = 0.5,\n",
    "        activation: str = \"relu\",\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.bias = bias\n",
    "\n",
    "        if self.activation == \"elu\":\n",
    "            self.activation = nn.ELU(inplace=True)\n",
    "\n",
    "        elif self.activation == \"gelu\":\n",
    "            self.activation = nn.GELU()\n",
    "\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            self.activation = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        else:\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        for index in range(2):\n",
    "            self.layers.append(\n",
    "                nn.Linear(\n",
    "                    in_features=self.in_features,\n",
    "                    out_features=self.out_features,\n",
    "                    bias=self.bias,\n",
    "                )\n",
    "            )\n",
    "            self.in_features = self.out_features\n",
    "            self.out_features = in_features\n",
    "\n",
    "            if index == 0:\n",
    "                self.layers.append(self.activation)\n",
    "                self.layers.append(nn.Dropout(p=self.dropout))\n",
    "\n",
    "        self.network = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return self.network(x)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Pointwise Feed Forward Network for Transformer\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--in_features\",\n",
    "        type=int,\n",
    "        default=config()[\"ViT\"][\"dimension\"],\n",
    "        help=\"Number of input features\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_features\",\n",
    "        type=int,\n",
    "        default=config()[\"ViT\"][\"dim_feedforward\"],\n",
    "        help=\"Number of output features\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--activation\",\n",
    "        type=str,\n",
    "        default=\"gelu\",\n",
    "        choices=[\"gelu\", \"relu\", \"silu\", \"leaky_relu\", \"elu\"],\n",
    "        help=\"Activation function\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    batch_size = config()[\"ViT\"][\"batch_size\"]\n",
    "    dimension = args.in_features\n",
    "\n",
    "    x = torch.randn((batch_size, 200, dimension))\n",
    "\n",
    "    net = FeedForwardNetwork(\n",
    "        in_features=args.in_features,\n",
    "        out_features=args.out_features,\n",
    "        activation=args.activation,\n",
    "        bias=True,\n",
    "    )\n",
    "\n",
    "    assert net(x=x).size() == (\n",
    "        batch_size,\n",
    "        200,\n",
    "        dimension,\n",
    "    ), \"Output shape is incorrect in PointWise FeedForward Network\".capitalize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimension: int = 512,\n",
    "        nheads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        dim_feedforward: int = 2048,\n",
    "        epsilon: float = 1e-5,\n",
    "        activation: str = \"relu\",\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.nheads = nheads\n",
    "        self.dropout = dropout\n",
    "        self.epsilon = epsilon\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.activation = activation\n",
    "        self.bias = bias\n",
    "\n",
    "        self.multihead_attention = MultiHeadAttention(\n",
    "            dimension=self.dimension,\n",
    "            nheads=self.nheads,\n",
    "            dropout=self.dropout,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "        self.feedforward_network = FeedForwardNetwork(\n",
    "            in_features=self.dimension,\n",
    "            out_features=self.dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "            activation=self.activation,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "        self.layernorm = LayerNormalization(\n",
    "            normalized_shape=self.dimension, epsilon=self.epsilon\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            residual = x\n",
    "\n",
    "            x = self.multihead_attention(x=x, mask=mask)\n",
    "            x = torch.dropout(input=x, p=self.dropout, train=self.training)\n",
    "            x = torch.add(x, residual)\n",
    "            x = self.layernorm(x)\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            x = self.feedforward_network(x=x)\n",
    "            x = torch.dropout(input=x, p=self.dropout, train=self.training)\n",
    "            x = torch.add(x, residual)\n",
    "            x = self.layernorm(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Encoder Block for the Transformer\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dimension\",\n",
    "        type=int,\n",
    "        default=config()[\"ViT\"][\"dimension\"],\n",
    "        help=\"Dimension of the input tensor\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nheads\",\n",
    "        type=int,\n",
    "        default=config()[\"ViT\"][\"nheads\"],\n",
    "        help=\"Number of heads in the multi-head attention\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dim_feedforward\",\n",
    "        type=int,\n",
    "        default=config()[\"ViT\"][\"dim_feedforward\"],\n",
    "        help=\"Dimension of the feedforward network\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\",\n",
    "        type=float,\n",
    "        default=config()[\"ViT\"][\"dropout\"],\n",
    "        help=\"Dropout rate\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--activation\",\n",
    "        type=str,\n",
    "        default=config()[\"ViT\"][\"activation\"],\n",
    "        help=\"Activation function\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eps\",\n",
    "        type=float,\n",
    "        default=config()[\"ViT\"][\"eps\"],\n",
    "        help=\"Epsilon value for the layer normalization\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    batch_size = config()[\"ViT\"][\"batch_size\"]\n",
    "\n",
    "    transformerEncoder = TransformerEncoderBlock(\n",
    "        dimension=args.dimension,\n",
    "        nheads=args.nheads,\n",
    "        dim_feedforward=args.dim_feedforward,\n",
    "        dropout=args.dropout,\n",
    "        activation=args.activation,\n",
    "        bias=True,\n",
    "        epsilon=args.eps,\n",
    "    )\n",
    "\n",
    "    assert transformerEncoder(torch.randn(batch_size, 200, args.dimension)).size() == (\n",
    "        batch_size,\n",
    "        200,\n",
    "        args.dimension,\n",
    "    ), \"Encoder block is not working properly\".capitalize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer - Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimension: int = 512,\n",
    "        nheads: int = 8,\n",
    "        num_encoder_layers: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        dim_feedforward: int = 2048,\n",
    "        epsilon: float = 1e-5,\n",
    "        activation: str = \"relu\",\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.nheads = nheads\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.dropout = dropout\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.epsilon = epsilon\n",
    "        self.activation = activation\n",
    "        self.bias = bias\n",
    "\n",
    "        self.transformerEncoder = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoderBlock(\n",
    "                    dimension=self.dimension,\n",
    "                    nheads=self.nheads,\n",
    "                    dropout=self.dropout,\n",
    "                    dim_feedforward=self.dim_feedforward,\n",
    "                    epsilon=self.epsilon,\n",
    "                    activation=self.activation,\n",
    "                    bias=self.bias,\n",
    "                )\n",
    "                for _ in range(self.num_encoder_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            for layer in self.transformerEncoder:\n",
    "                x = layer(x=x, mask=mask)\n",
    "\n",
    "            return x\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Transformer Encoder block for ViT\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dimension\",\n",
    "        type=int,\n",
    "        default=config()[\"ViT\"][\"dimension\"],\n",
    "        help=\"Dimension of the input tensor\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nheads\",\n",
    "        type=int,\n",
    "        default=config()[\"ViT\"][\"nheads\"],\n",
    "        help=\"Number of heads in the multi-head attention\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dim_feedforward\",\n",
    "        type=int,\n",
    "        default=config()[\"ViT\"][\"dim_feedforward\"],\n",
    "        help=\"Dimension of the feedforward network\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\",\n",
    "        type=float,\n",
    "        default=config()[\"ViT\"][\"dropout\"],\n",
    "        help=\"Dropout rate\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--activation\",\n",
    "        type=str,\n",
    "        default=config()[\"ViT\"][\"activation\"],\n",
    "        help=\"Activation function\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eps\",\n",
    "        type=float,\n",
    "        default=config()[\"ViT\"][\"eps\"],\n",
    "        help=\"Epsilon value for the layer normalization\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_layers\",\n",
    "        type=int,\n",
    "        default=config()[\"ViT\"][\"num_layers\"],\n",
    "        help=\"Number of layers in the transformer encoder\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    batch_size = config()[\"ViT\"][\"batch_size\"]\n",
    "\n",
    "    transformerEncoder = TransformerEncoder(\n",
    "        dimension=args.dimension,\n",
    "        nheads=args.nheads,\n",
    "        dim_feedforward=args.dim_feedforward,\n",
    "        num_encoder_layers=args.num_layers,\n",
    "        dropout=args.dropout,\n",
    "        activation=args.activation,\n",
    "        bias=True,\n",
    "        epsilon=args.eps,\n",
    "    )\n",
    "\n",
    "    assert transformerEncoder(torch.randn(batch_size, 200, 512)).size() == (\n",
    "        batch_size,\n",
    "        200,\n",
    "        args.dimension,\n",
    "    ), \"TransformerEncoder output size is incorrect\"\n",
    "\n",
    "    print(\"TransformerEncoder test passed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
